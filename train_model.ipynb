{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "hf_cGmzeQvNO"
      },
      "outputs": [],
      "source": [
        "# Importing required libraries\n",
        "import nltk  # Natural Language Toolkit for text processing\n",
        "import pickle  # To save and load model files\n",
        "from sklearn.model_selection import train_test_split  # To split data into training and testing\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer  # To convert text into numerical features\n",
        "from sklearn.naive_bayes import MultinomialNB  # Naive Bayes model for classification\n",
        "from sklearn.metrics import accuracy_score, classification_report  # To check model performance\n",
        "import pandas as pd  # For handling dataset\n",
        "import re  # Regular expressions for text cleaning\n",
        "from nltk.tokenize import word_tokenize  # To split sentences into words\n",
        "from nltk.stem import WordNetLemmatizer  # To reduce words to their base form\n",
        "from imblearn.over_sampling import SMOTE  # To handle imbalanced data by generating synthetic samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E0tkSh-QuW3s",
        "outputId": "573a3d00-c3da-44a7-8529-007637aeb034"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\conne\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\conne\\AppData\\Roaming\\nltk_data...\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Downloading necessary NLTK data files\n",
        "nltk.download('punkt')  # For tokenization\n",
        "nltk.download('wordnet')  # For lemmatization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VvNWoiuDRjlc",
        "outputId": "fd5dc38d-d1aa-4445-ef67-896d8b03ae76"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Columns in dataset: ['Comment', 'Sentiment']\n"
          ]
        }
      ],
      "source": [
        "# Load the dataset\n",
        "df = pd.read_csv('data/YoutubeCommentsDataSet.csv')  # Read the CSV file containing YouTube comments\n",
        "print(\"Columns in dataset:\", df.columns.tolist())  # Print the column names of the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "K-7DotCZ0J5k"
      },
      "outputs": [],
      "source": [
        "# Drop rows where 'Comment' or 'Sentiment' is missing (null)\n",
        "df.dropna(subset=['Comment', 'Sentiment'], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "om2odOjGRnzo"
      },
      "outputs": [],
      "source": [
        "# Function to clean the text data\n",
        "def clean_text(text):\n",
        "    text = str(text)  # Make sure the input is a string\n",
        "    text = re.sub(r'http\\S+', '', text)  # Remove any URLs from text\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove all special characters and numbers\n",
        "    return text.strip().lower()  # Remove extra spaces and convert text to lowercase"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "9vF5JJWmRuGP"
      },
      "outputs": [],
      "source": [
        "# Apply text cleaning to the 'Comment' column\n",
        "df['clean_text'] = df['Comment'].apply(clean_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "NvHKC1vMRy6r"
      },
      "outputs": [],
      "source": [
        "# Initialize the lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Apply lemmatization on the cleaned text\n",
        "df['lemmatized_tokens'] = df['clean_text'].apply(\n",
        "    lambda x: [lemmatizer.lemmatize(word) for word in word_tokenize(x)]  # Lemmatize each word\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "EEcPO68sSLlc"
      },
      "outputs": [],
      "source": [
        "# Initialize TF-IDF Vectorizer to convert text into numerical form\n",
        "vectorizer = TfidfVectorizer(\n",
        "    stop_words='english',  # Remove common English stopwords\n",
        "    ngram_range=(1, 2),  # Use both unigrams and bigrams\n",
        "    sublinear_tf=True,  # Apply sublinear term frequency scaling\n",
        "    max_df=0.95,  # Ignore words that appear in more than 95% of documents\n",
        "    min_df=5  # Ignore words that appear in less than 5 documents\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "zqVL_KQ9v5cs"
      },
      "outputs": [],
      "source": [
        "# Transform the cleaned text into vectors\n",
        "X = vectorizer.fit_transform(df['clean_text'])  # Features (X)\n",
        "y = df['Sentiment']  # Target variable (y)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ZsXiky6zQcw1"
      },
      "outputs": [],
      "source": [
        "# Apply SMOTE to balance the classes (if some sentiments have very few samples)\n",
        "smote = SMOTE(random_state=42)\n",
        "X_resampled, y_resampled = smote.fit_resample(X, y)  # Generate synthetic samples\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "PkcARwhewY9-"
      },
      "outputs": [],
      "source": [
        "# Split the data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_resampled, y_resampled, test_size=0.2, random_state=42\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "3iFB8HoAwe0U",
        "outputId": "84d08301-70f2-4129-d10f-a491334b95ee"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "model = MultinomialNB()\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "wS3TXZ_Qwn2C"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Predict sentiments for test data\n",
        "y_pred = model.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PMP8_8jv1pJ0",
        "outputId": "120cc8df-4cf6-4644-f543-b1e879f29fab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.7392575270388775\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.66      0.94      0.77      2222\n",
            "     neutral       0.84      0.49      0.61      2287\n",
            "    positive       0.79      0.80      0.80      2333\n",
            "\n",
            "    accuracy                           0.74      6842\n",
            "   macro avg       0.76      0.74      0.73      6842\n",
            "weighted avg       0.76      0.74      0.73      6842\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Check how well the model is performing\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))  # Print accuracy\n",
        "print(classification_report(y_test, y_pred))  # Print detailed classification report\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "07RVAjXA1x70"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Save the trained model using pickle\n",
        "with open(\"sentiment_model.pkl\", \"wb\") as model_file:\n",
        "    pickle.dump(model, model_file)\n",
        "\n",
        "# Save the TF-IDF vectorizer as well\n",
        "with open(\"tfidf_vectorizer.pkl\", \"wb\") as vectorizer_file:\n",
        "    pickle.dump(vectorizer, vectorizer_file)\n",
        "\n",
        "# Function to load the saved model and vectorizer later for prediction\n",
        "def load_model():\n",
        "    with open(\"sentiment_model.pkl\", \"rb\") as model_file:\n",
        "        loaded_model = pickle.load(model_file)  # Load the model\n",
        "    with open(\"tfidf_vectorizer.pkl\", \"rb\") as vectorizer_file:\n",
        "        loaded_vectorizer = pickle.load(vectorizer_file)  # Load the vectorizer\n",
        "    return loaded_model, loaded_vectorizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "_zZDlYcF2cmZ"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Predict the sentiment of all comments in the dataset\n",
        "df['predicted_sentiment'] = model.predict(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T-GP1Ti52fhM",
        "outputId": "0d723789-1a67-476b-df64-e3cf93799553"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                             Comment Sentiment  \\\n",
            "0  lets not forget that apple pay in 2014 require...   neutral   \n",
            "1  here in nz 50 of retailers don’t even have con...  negative   \n",
            "2  i will forever acknowledge this channel with t...  positive   \n",
            "3  whenever i go to a place that doesn’t take app...  negative   \n",
            "4  apple pay is so convenient secure and easy to ...  positive   \n",
            "\n",
            "  predicted_sentiment  \n",
            "0            negative  \n",
            "1            negative  \n",
            "2            positive  \n",
            "3            negative  \n",
            "4            negative  \n"
          ]
        }
      ],
      "source": [
        "# Show few rows with original and predicted sentiments\n",
        "print(df[['Comment', 'Sentiment', 'predicted_sentiment']].head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "kp_Ckw0J2k7w"
      },
      "outputs": [],
      "source": [
        "# Save the updated dataset (with predictions) into a new CSV file\n",
        "df.to_csv(\"YoutubeCommentsDataSet_with_predictions.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "CafcmnR-wtTA"
      },
      "outputs": [],
      "source": [
        "# Define a function to predict sentiment for any new comment\n",
        "def predict_sentiment(comment):\n",
        "    model, vectorizer = load_model()  # Load the saved model and vectorizer\n",
        "    clean_comment = clean_text(comment)  # Clean the new comment\n",
        "    vectorized_comment = vectorizer.transform([clean_comment])  # Convert it into vector form\n",
        "    return model.predict(vectorized_comment)[0]  # Return the predicted sentiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_rjOsDww4Bu",
        "outputId": "de591f78-7460-41ca-a2b6-5b667310c1b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted Sentiment: negative\n"
          ]
        }
      ],
      "source": [
        "# Example usage of prediction function\n",
        "example_comment = \"This video is worst\"\n",
        "print(\"Predicted Sentiment:\", predict_sentiment(example_comment))  # Predict and print sentiment"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
